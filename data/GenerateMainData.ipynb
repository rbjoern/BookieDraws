{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate main data from external files\n",
    "The file loads, processes and combines the external data files to form the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#The code has the following sections: \n",
    "    # Generate main table\n",
    "    # Weather for each match\n",
    "    # Elo ratings\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "### GENERATE MAIN TABLE\n",
    "match = pd.read_csv(os.getcwd() + '\\\\raw_data\\\\European_soccer\\\\Match.csv', index_col=0) #Load CSV file retrieved from SQLite database\n",
    "\n",
    "#main_df = match.iloc[:1000,0:8] #TEMPORARY: Work with top 1000 rows for speed. \n",
    "main_df = match.iloc[:,0:8] #First columns are various ID columns we might need. \n",
    "\n",
    "\n",
    "###############################################################################\n",
    "### THE ENDOGENOUS SIDE OF THINGS\n",
    "main_df['draw'] = (match['home_team_goal']==match['away_team_goal']) #Dummy for ties\n",
    "main_df['home'] = (match['home_team_goal']>match['away_team_goal']) #Dummy for ties\n",
    "main_df['away'] = (match['home_team_goal']<match['away_team_goal']) #Dummy for ties\n",
    "\n",
    "#Compute average odds \n",
    "main_df['odds_draw'] = match[['B365D','BWD','IWD','LBD','PSD','WHD','SJD','VCD','GBD','BSD']].max(axis=1,skipna=True)\n",
    "main_df['odds_home'] = match[['B365H','BWH','IWH','LBH','PSH','WHH','SJH','VCH','GBH','BSH']].max(axis=1,skipna=True)\n",
    "main_df['odds_away'] = match[['B365A','BWA','IWA','LBA','PSA','WHA','SJA','VCA','GBA','BSA']].max(axis=1,skipna=True)\n",
    "\n",
    "\n",
    "#Compute average odds-probabilities\n",
    "def prob_from_odds(odds): \n",
    "    prob = (1/odds) #lol\n",
    "    return prob\n",
    "\n",
    "main_df['odds_prob_draw'] = prob_from_odds(main_df['odds_draw'])\n",
    "main_df['odds_prob_home'] = prob_from_odds(main_df['odds_home'])\n",
    "main_df['odds_prob_away'] = prob_from_odds(main_df['odds_away'])\n",
    "\n",
    "#print('Share of matches ending in a draw: ', round(main_df['draw'].mean(),2))\n",
    "#print('Average odds probability of a draw:', round(main_df['odds_prob_draw'].mean(),2))\n",
    "\n",
    "###############################################################################\n",
    "### THE EXOGENOUS SIDE OF THINGS\n",
    "\n",
    "### CONVERT ID VARIABLES TO NICELY READABLE STUFF FOR BOTH HUMANS AND MACHINES\n",
    "teams =  pd.read_csv(os.getcwd() + '\\\\raw_data\\\\European_soccer\\\\Team.csv', index_col='team_api_id') #Index column api\n",
    "main_df[['team_home','team_home_s']] = main_df.join(teams, on='home_team_api_id')[['team_long_name',\t'team_short_name']]\n",
    "main_df[['team_awat','team_away_s']] = main_df.join(teams, on='away_team_api_id')[['team_long_name',\t'team_short_name']]\n",
    "del teams #Housekeeping\n",
    "\n",
    "country = pd.read_csv(os.getcwd() + '\\\\raw_data\\\\European_soccer\\\\Country.csv', index_col='id') #Index column api\n",
    "main_df['country'] = main_df.join(country, on='country_id')[['name']]\n",
    "del country\n",
    "\n",
    "league = pd.read_csv(os.getcwd() + '\\\\raw_data\\\\European_soccer\\\\League.csv', index_col='id') #Index column api\n",
    "main_df['league'] = main_df.join(league[['name']], on='league_id')[['name']]\n",
    "del league\n",
    "\n",
    "#Housekeeping: Delete id variables after use\n",
    "main_df.drop(labels=['country_id', 'league_id','stage','home_team_api_id','away_team_api_id'], axis=1, inplace=True)\n",
    "\n",
    "### COMPUTE FEATURES BASED ON PREVIOUS GAMES \n",
    "main_df.sort_values('date',inplace=True)\n",
    "\n",
    "#Set parameters for the rolling stuf\n",
    "rolling_periods = 20\n",
    "rolling_min = rolling_periods//2 \n",
    "\n",
    "#Create a temporary dataframe we can play around with. \n",
    "match2 = match[['date','match_api_id','home_team_api_id','away_team_api_id','home_team_goal', 'away_team_goal']].sort_values('date')#.groupby('home_team_api_id')\n",
    "\n",
    "\n",
    "### Aggression and defense seperately for home and away games\n",
    "# Home aggression: Average goals in last few home games. \n",
    "# Compute as rolling average. Unfortunately, this includes current period. Hence, we shift the result one row down. \n",
    "match2['home_agg'] = match2.groupby('home_team_api_id')['home_team_goal']\\\n",
    "                    .rolling(rolling_periods, min_periods=rolling_min).mean().shift(1).reset_index(0,drop=True) #reset is technical, in order to return a series. Weird. \n",
    "#Unfortunately the  shifting moves the rolling average from the last row to the first. We'll just remove it.                   \n",
    "match2.loc[match2.groupby('home_team_api_id')['home_agg'].head(1).index, 'home_agg'] = np.nan\n",
    "#match3 = match2[match2['home_team_api_id']==1957].copy() #Tjek\n",
    "\n",
    "# Home defensiveness: Average goals on themselves in last few home games. \n",
    "match2['home_def'] = match2.groupby('home_team_api_id')['away_team_goal']\\\n",
    "                    .rolling(rolling_periods, min_periods=rolling_min).mean().shift(1).reset_index(0,drop=True) \n",
    "match2.loc[match2.groupby('home_team_api_id')['home_def'].head(1).index, 'home_def'] = np.nan\n",
    "#match3 = match2[match2['home_team_api_id']==2183].copy() #Tjek\n",
    "\n",
    "# Away aggression: Average goals in last few away games. \n",
    "match2['away_agg'] = match2.groupby('away_team_api_id')['away_team_goal']\\\n",
    "                    .rolling(rolling_periods, min_periods=rolling_min).mean().shift(1).reset_index(0,drop=True) \n",
    "match2.loc[match2.groupby('away_team_api_id')['away_agg'].head(1).index, 'away_agg'] = np.nan\n",
    "#match3 = match2[match2['away_team_api_id']==4049].copy() #Tjek\n",
    "\n",
    "# Away defensiveness: Average goals in last few away games. \n",
    "match2['away_def'] = match2.groupby('away_team_api_id')['home_team_goal']\\\n",
    "                    .rolling(rolling_periods, min_periods=rolling_min).mean().shift(1).reset_index(0,drop=True) \n",
    "match2.loc[match2.groupby('away_team_api_id')['away_def'].head(1).index, 'away_def'] = np.nan\n",
    "#match3 = match2[match2['away_team_api_id']==6269].copy() #Tjek\n",
    "\n",
    "\n",
    "### Aggression and defense pooled for home and away games\n",
    "#Measures for home team \n",
    "match2['home_agg_overall'] = np.nan\n",
    "match2['home_def_overall'] = np.nan\n",
    "\n",
    "for team in match2['home_team_api_id'].unique(): \n",
    "    #Get a teams goals whether they were away or home\n",
    "    data = match2.loc[(match2['home_team_api_id'] == team), \n",
    "                      ['date', 'home_team_goal', 'away_team_goal']]\\\n",
    "            .rename(columns={ 'home_team_goal': 'own_goals', 'away_team_goal': 'their_goals'})\n",
    "    data = data.append(match2.loc[(match2['away_team_api_id'] == team), \n",
    "                                  ['date', 'away_team_goal', 'home_team_goal']]\\\n",
    "            .rename(columns={ 'away_team_goal': 'own_goals', 'home_team_goal': 'their_goals'}))\n",
    "    \n",
    "    #Resort, so matches are in date orer\n",
    "    data.sort_values(by='date', ascending=True, inplace=True)\n",
    "    \n",
    "    #Compute measures based on læst games no matter whether they were away or at home\n",
    "    data['home_agg_overall'] = data['own_goals']\\\n",
    "                                .rolling(rolling_periods, min_periods=rolling_min).mean().shift(1)\n",
    "    data['home_def_overall'] = data['their_goals']\\\n",
    "                                .rolling(rolling_periods, min_periods=rolling_min).mean().shift(1)\n",
    "                                \n",
    "    match2.loc[(match2['home_team_api_id'] == team), 'home_agg_overall'] = data['home_agg_overall']\n",
    "    match2.loc[(match2['home_team_api_id'] == team), 'home_def_overall'] = data['home_def_overall']\n",
    "del team, data\n",
    "\n",
    "#Measures for away team \n",
    "match2['away_agg_overall'] = np.nan\n",
    "match2['away_def_overall'] = np.nan\n",
    "\n",
    "for team in match2['away_team_api_id'].unique(): \n",
    "    #Get a teams goals whether they were away or home\n",
    "    data = match2.loc[(match2['away_team_api_id'] == team), \n",
    "                      ['date', 'away_team_goal', 'home_team_goal']]\\\n",
    "            .rename(columns={ 'away_team_goal': 'own_goals', 'home_team_goal': 'their_goals'})\n",
    "    data = data.append(match2.loc[(match2['home_team_api_id'] == team), \n",
    "                                  ['date', 'home_team_goal', 'away_team_goal']]\\\n",
    "            .rename(columns={'home_team_goal': 'own_goals', 'away_team_goal': 'their_goals'}))\n",
    "    \n",
    "    #Resort, so matches are in date orer\n",
    "    data.sort_values(by='date', ascending=True, inplace=True)\n",
    "    \n",
    "    #Compute measures based on læst games no matter whether they were away or at home\n",
    "    data['away_agg_overall'] = data['own_goals']\\\n",
    "                                .rolling(rolling_periods, min_periods=rolling_min).mean().shift(1)\n",
    "    data['away_def_overall'] = data['their_goals']\\\n",
    "                                .rolling(rolling_periods, min_periods=rolling_min).mean().shift(1)\n",
    "                                \n",
    "    match2.loc[(match2['away_team_api_id'] == team), 'away_agg_overall'] = data['away_agg_overall']\n",
    "    match2.loc[(match2['away_team_api_id'] == team), 'away_def_overall'] = data['away_def_overall']\n",
    "del team, data\n",
    "\n",
    "#### Home advantage and away disadvantage\n",
    "#match2['home_adv'] = match2\n",
    "\n",
    "\n",
    "del rolling_periods, rolling_min\n",
    "\n",
    "#Lets fold it back into the main dataframe \n",
    "match2.drop(labels=['home_team_goal', 'away_team_goal', 'home_team_api_id','away_team_api_id'], axis=1, inplace=True)\n",
    "main_df = main_df.merge(match2, on=['match_api_id', 'date'], how='left')\n",
    "del match, match2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "### WEATHER FOR EACH MATCH\n",
    "\n",
    "###############################################################################\n",
    "### STADIUM COORDINATES\n",
    "#import r-wikidatar\n",
    "import requests\n",
    "url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "query = '''SELECT ?club ?clubLabel ?venue ?venueLabel ?coordinates\n",
    "WHERE\n",
    "{\n",
    "\t?club wdt:P31 wd:Q476028 .\n",
    "\t?club wdt:P115 ?venue .\n",
    "\t?venue wdt:P625 ?coordinates .\n",
    "\tSERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "}'''\n",
    "data = requests.get(url, params={'query': query, 'format': 'json'}).json()\n",
    "stadiums = []   \n",
    "for item in data['results']['bindings']: \n",
    "    stadiums.append({\n",
    "            'club': item['club']['value'],\n",
    "            'team_home': item['clubLabel']['value'],\n",
    "            'venue': item['venue']['value'],\n",
    "            'venueLabel': item['venueLabel']['value'],\n",
    "            'coordinates': item['coordinates']['value'],\n",
    "                })\n",
    "stadiums = pd.DataFrame(stadiums)\n",
    "del query, url, data, item \n",
    " \n",
    "import unidecode\n",
    "\n",
    "def clean_names(input_names): #Input must be pandas series\n",
    "    vfunc = np.vectorize(unidecode.unidecode)\n",
    "    output_names =  pd.Series(vfunc(input_names)) #Remove special characters\n",
    "    output_names = output_names.str.lower() #Lowercase\n",
    "    output_names = output_names.replace('[\\.]','', regex=True) #Remove periods\n",
    "    output_names = output_names.replace('[\\d]','', regex=True) #Remove numbers\n",
    "    output_names = output_names.replace('[-]',' ', regex=True)\n",
    "    output_names = output_names.replace('afc','', regex=True)\n",
    "    output_names = output_names.replace('fc','', regex=True)\n",
    "    output_names = output_names.replace('uc','', regex=True)\n",
    "    output_names = output_names.replace('ksv','', regex=True)\n",
    "    output_names = output_names.replace('club','', regex=True)\n",
    "#   output_names = output_names.replace('tsv','', regex=True)\n",
    "#    output_names = output_names.replace('nk','', regex=True)\n",
    "#    output_names = output_names.replace('sg','', regex=True)\n",
    "    output_names = output_names.replace('rc','', regex=True)\n",
    "#    output_names = output_names.replace('osc','', regex=True)\n",
    "#    output_names = output_names.replace('usl','', regex=True)\n",
    "#    output_names = output_names.replace('sm','', regex=True)\n",
    "    output_names = output_names.replace('cd','', regex=True)\n",
    "#    output_names = output_names.replace('cp','', regex=True)\n",
    "    #output_names = output_names.replace('[z]','', regex=True)\n",
    "    #output_names = output_names.replace('cbd','', regex=True)\n",
    "    \n",
    "    output_names = output_names.str.strip() #Trim whitespace \n",
    "    return output_names\n",
    "\n",
    "#Manuel adjustments\n",
    "navn1= [('FCV Dender EH','F.C. Verbroedering Dender Eendracht Hekelgem'),\n",
    "        ('P. Warszawa','Polonia Warsaw'),\n",
    "    ('Hertha BSC Berlin','Hertha BSC'),\n",
    "    ('Sporting Charleroi','R. Charleroi S.C.'),\n",
    "    ('Sporting Lokeren','K.S.C. Lokeren Oost-Vlaanderen'),\n",
    "    ('Chievo Verona','A.C. ChievoVerona'),\n",
    "    ('Athletic Club de Bilbao','Athletic Bilbao B'),\n",
    "    ('US Boulogne Cote D\\'Opale','US Boulogne'),\n",
    "    ('Xerez Club Deportivo','Xerez C.D.'),\n",
    "    ('Termalica Bruk-Bet Nieciecza','LKS Nieciecza'), \n",
    "    ('Legia Warszawa','Legia Warsaw'),\n",
    "    ('Cracovia','KS Cracovia'),\n",
    "    ('DSC Arminia Bielefeld','Arminia Bielefeld'),\n",
    "    ('LOSC Lille','Lille O.S.C.'),\n",
    "    ('SM Caen','Stade Malherbe Caen'),\n",
    "    ('Dundee United','Dundee F.C.'),\n",
    "    ('Hannover 96','OSV Hannover'),\n",
    "    ('Hamburger SV','SC Victoria Hamburg'),\n",
    "    ('Amadora','C.F. Estrela da Amadora'),\n",
    "    ('Sampdoria','Unione Calcio Sampdoria'),\n",
    "    ('Udinese','Udinese Calcio'),\n",
    "    ('Atalanta','Atalanta B.C.'),\n",
    "    ('Milan','A.C. Milan'),\n",
    "    ('Roma','A.S. Roma'),\n",
    "    ('Racing Santander','Racing de Santander'),\n",
    "    ('CD Numancia','Club Deportivo Numancia'),\n",
    "    ('Torino','Torino Football Club'),\n",
    "    ('Catania','Calcio Catania'),\n",
    "    ('Lecce','Unione Sportiva Lecce'),\n",
    "    ('Lazio','S.S. Lazio'),\n",
    "    ('Genoa','Genoa Cricket and Football Club'),\n",
    "    ('Livorno','Associazione Sportiva Livorno Calcio'),\n",
    "    ('Parma','Parma Calcio 1913'),\n",
    "    ('AC Arles-Avignon','Avignon Foot 84'),\n",
    "    ('Excelsior','SBV Excelsior'),\n",
    "    ('Cesena','A.C. Cesena'),\n",
    "    ('Brescia','Brescia Calcio'),\n",
    "    ('FC Lausanne-Sports','FC Lausanne-Sport'),\n",
    "    ('FC Augsburg','BC Augsburg'),\n",
    "    ('Novara','Novara Calcio'),\n",
    "    ('RC Celta de Vigo','Celta Vigo B'),\n",
    "    ('Estoril Praia','G.D. Estoril Praia'),\n",
    "    ('Pescara','Delfino Pescara 1936'),\n",
    "    ('Sassuolo','U.S. Sassuolo Calcio'),\n",
    "    ('KV Oostende','A.S.V. Oostende K.M.'),\n",
    "    ('Watford','Watford Football Club'),\n",
    "    #('Uniao da Madeira','Uniao Flamengo Santos F.C.'),\n",
    "    ('Frosinone','Frosinone Calcio'),\n",
    "    ('FC Basel','FC Concordia Basel'),\n",
    "]\n",
    "\n",
    "navn2= [('vitoria guimaraes','vitoria de guimaraes b'),\n",
    "('standard de liege','standard liege'),\n",
    "#('sporting cp','sporting e petroleos de cabinda'),\n",
    "('naval deg de maio','associacao naval o de maio'),\n",
    "('az','az alkmaar'),\n",
    "('fiorentina','acf fiorentina'),\n",
    "('academica de coimbra','associacao academica de coimbra   oaf'),\n",
    "('vitoria setubal','vitoria'),\n",
    "('real sporting de gijon','sporting gijon'),\n",
    "('psv','eindhoven'),\n",
    "('palermo','societa di calcio palermo'),\n",
    "('inter','intermierdale de milano'),\n",
    "('recreativo','recreativo de huelva'),\n",
    "('villarreal cf','villarreal  de futbol'),\n",
    "('uniao de leiria, sad','ud leiria'),\n",
    "('sc beira mar','sc beira mar vale callampa'),\n",
    "('dundee','dundee football'),\n",
    "('g ajaccio','gazelec ajaccio'),\n",
    "\n",
    "]\n",
    "\n",
    "for navn in navn1: #Manual replacements of some team names \n",
    "    stadiums['team_home'].replace(navn[1], navn[0], inplace=True)\n",
    "del navn, navn1\n",
    "\n",
    "main_df['funky'] = clean_names(main_df['team_home'])\n",
    "stadiums['funky'] = clean_names(stadiums['team_home'])\n",
    "\n",
    "for navn in navn2: #Manual replacements of some team names \n",
    "    stadiums['funky'].replace(navn[1], navn[0], inplace=True)\n",
    "del navn, navn2\n",
    "\n",
    "stadiums.drop_duplicates(subset=['funky'], keep='first', inplace=True)\n",
    "stadiums.to_csv(os.getcwd() + '\\\\work_files\\\\stadiums.csv')\n",
    "stadiums.drop(labels=['team_home'], axis=1, inplace=True)\n",
    "\n",
    "main_df = main_df.merge(stadiums, on='funky', how='left')\n",
    "main_df2 = main_df.merge(stadiums, on='funky', how='inner')\n",
    "a = main_df[~main_df['funky'].isin(main_df2['funky'])][['team_home','funky']].drop_duplicates()\n",
    "a.to_csv(os.getcwd() + '\\\\work_files\\\\manglende_hold.csv')\n",
    "del a, main_df2\n",
    "\n",
    "main_df.drop(labels=['funky', 'club',  'venue'], axis=1, inplace=True)\n",
    "del stadiums \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share of nulls (before changes): <xarray.DataArray 'rr' ()>\n",
      "array(0.713535)\n",
      "Share of nulls (ffill latitude): <xarray.DataArray 'rr' ()>\n",
      "array(0.668724)\n",
      "Share of nulls (ffill longitude) <xarray.DataArray 'rr' ()>\n",
      "array(0.637891)\n",
      "Share of nulls (bfill latitude): <xarray.DataArray 'rr' ()>\n",
      "array(0.605013)\n",
      "Share of nulls (bfill longitude) <xarray.DataArray 'rr' ()>\n",
      "array(0.58326)\n",
      "Updated for row 0\n",
      "Updated for row 1000\n",
      "Updated for row 2000\n",
      "Updated for row 3000\n",
      "Updated for row 4000\n",
      "Updated for row 5000\n",
      "Updated for row 6000\n",
      "Updated for row 7000\n",
      "Updated for row 8000\n",
      "Updated for row 9000\n",
      "Updated for row 10000\n",
      "Updated for row 11000\n",
      "Updated for row 12000\n",
      "Updated for row 13000\n",
      "Updated for row 14000\n",
      "Updated for row 15000\n",
      "Updated for row 16000\n",
      "Updated for row 17000\n",
      "Updated for row 18000\n",
      "Updated for row 19000\n",
      "Updated for row 20000\n",
      "Updated for row 21000\n",
      "Updated for row 22000\n",
      "Updated for row 23000\n",
      "Updated for row 24000\n",
      "Updated for row 25000\n",
      "Share of nulls (RR): 0.08021863813079795\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "### WEATHER DATA\n",
    "#import netCDF4\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import gzip\n",
    "#from shapely.geometry import Point\n",
    "\n",
    "\n",
    "# https://www.ecad.eu/download/ensembles/downloadchunks.php\n",
    "with gzip.open(os.getcwd() + '\\\\raw_data\\\\Weather\\\\rr_0.25deg_reg_2011-2017_v17.0.nc.gz') as f: \n",
    "    weather_2011 = xr.open_dataset(f)\n",
    "with gzip.open(os.getcwd() + '\\\\raw_data\\\\Weather\\\\rr_0.25deg_reg_1995-2010_v17.0.nc.gz') as f: \n",
    "    weather_1995 = xr.open_dataset(f)    \n",
    "weather = xr.concat([weather_1995,weather_2011], dim='time')\n",
    "#weather = weather.to_dataframe()\n",
    "del weather_2011, weather_1995\n",
    "\n",
    "#print('Share of nulls:', weather.isnull().sum()/len(weather))\n",
    "fill_limit = 4 \n",
    "print('Share of nulls (before changes):', weather.isnull().rr.values.sum()/(weather.isnull().rr.values.sum()+weather.rr.count()))\n",
    "weather = weather.ffill(dim='latitude', limit=fill_limit)\n",
    "print('Share of nulls (ffill latitude):', weather.isnull().rr.values.sum()/(weather.isnull().rr.values.sum()+weather.rr.count()))\n",
    "weather = weather.ffill(dim='longitude', limit=fill_limit)\n",
    "print('Share of nulls (ffill longitude)', weather.isnull().rr.values.sum()/(weather.isnull().rr.values.sum()+weather.rr.count()))\n",
    "weather = weather.bfill(dim='latitude', limit=fill_limit)\n",
    "print('Share of nulls (bfill latitude):', weather.isnull().rr.values.sum()/(weather.isnull().rr.values.sum()+weather.rr.count()))\n",
    "weather = weather.bfill(dim='longitude', limit=fill_limit)\n",
    "print('Share of nulls (bfill longitude)', weather.isnull().rr.values.sum()/(weather.isnull().rr.values.sum()+weather.rr.count()))\n",
    "del fill_limit\n",
    "\n",
    "#Tjekker\n",
    "#date = main_df['date'].str[:10][25000]\n",
    "#longitude = float(main_df['coordinates'][25000].split(\" \")[0].replace('Point(', ''))\n",
    "#latitude = float(main_df['coordinates'][25000].split(\" \")[1].replace(')', ''))\n",
    "#weather.sel(time=date, latitude=latitude, longitude=longitude, method='nearest')\n",
    " \n",
    "main_df['RR'] = np.nan\n",
    "#main_df['RR_date'] = np.nan\n",
    "#main_df['RR_lon'] = np.nan\n",
    "#main_df['RR_lat'] = np.nan\n",
    "\n",
    "\n",
    "main_df['date_s'] = main_df['date'].str[:10]\n",
    "main_df['longitude'], main_df['latitude'] = main_df['coordinates'].str.split(\" \").str #.replace('Point(', ''))\n",
    "main_df['longitude'] = main_df['longitude'].str.replace('Point\\(', '').astype(float)\n",
    "main_df['latitude'] = main_df['latitude'].str.replace('\\)', '').astype(float)\n",
    "\n",
    "\n",
    "for i in range(0,len(main_df)): #Find data in weather database, and fill in\n",
    "    if np.isnan(main_df['latitude'][i]) == False: \n",
    "        wea = weather.sel(time=main_df['date_s'][i], \n",
    "                          latitude=main_df['latitude'][i], \n",
    "                          longitude=main_df['longitude'][i], method='nearest')\n",
    "        main_df.loc[i, 'RR'] = wea.rr.values\n",
    "#        main_df.loc[i, 'RR_date'] = wea.time.values\n",
    "#        main_df.loc[i, 'RR_lat'] = wea.latitude.values\n",
    "#        main_df.loc[i, 'RR_lon'] = wea.longitude.values\n",
    "    if i % 1000 == 0: \n",
    "        print('Updated for row', i)\n",
    "del i, weather\n",
    "\n",
    "#Tjek: Share of nulls. \n",
    "print('Share of nulls (RR):', main_df['RR'].isnull().sum()/len(main_df['RR']))\n",
    "\n",
    "##Tjek: Passer datoerne?\n",
    "#print('Share of wrong dates (RR):', np.mean(main_df['date_s']!=main_df['RR_date'].str[:10]))\n",
    "#tjek = main_df[(main_df['date_s']!=main_df['RR_date'].str[:10])]\n",
    "#main_df.drop(labels=['RR_date', 'RR_lat', 'RR_lon'], axis=1, inplace=True)\n",
    "\n",
    "main_df.drop(labels=['date_s', 'longitude', 'latitude'], axis=1, inplace=True)\n",
    "#main_df.drop(labels=['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share of nulls (team): 0.298\n",
      "Finished inserting elo for team 50\n",
      "Finished inserting elo for team 100\n",
      "Finished inserting elo for team 150\n",
      "Finished inserting elo for team 200\n",
      "Finished inserting elo for team 250\n",
      "Share of nulls (Home ELO): 0.21\n",
      "Share of nulls (Away ELO): 0.21\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "### ELO RATINGS\n",
    "\n",
    "# Get the team names used in elo site\n",
    "teams =  pd.read_csv(os.getcwd() + '\\\\raw_data\\\\European_soccer\\\\Team.csv', index_col='team_api_id') #Index column api\n",
    "teams_elo = pd.read_csv(os.getcwd() + '\\\\work_files\\\\Elo_ratings\\\\Team_match.csv', encoding='latin-1')\n",
    "teams_elo.columns = ['elo_name', 'team_long_name']\n",
    "\n",
    "import unidecode \n",
    "def clean_names2(input_names): #Input must be pandas series\n",
    "    input_names = input_names.replace('[ü]', 'ue', regex=True)\n",
    "    input_names = input_names.replace('[ö]', 'oe', regex=True)\n",
    "    input_names = input_names.replace('[ä]', 'ae', regex=True)\n",
    "    input_names = input_names.replace('[.]', '', regex=True)\n",
    "    vfunc = np.vectorize(unidecode.unidecode)\n",
    "    output_names =  pd.Series(vfunc(input_names)) #Remove special characters\n",
    "    output_names = output_names.str.lower() #Lowercase\n",
    "    output_names = output_names.replace(' ','', regex=True) #Remove space inbetween words\n",
    "    return output_names\n",
    "\n",
    "teams_elo.elo_name = clean_names2(teams_elo.elo_name)\n",
    "teams_elo['elo_name'].replace('realsociedad', 'sociedad', inplace=True)\n",
    "\n",
    "teams = teams.merge(teams_elo, how='left', on='team_long_name')\n",
    "#Tjek\n",
    "print('Share of nulls (team):', round(teams['elo_name'].isnull().sum()/len(teams['team_long_name']),3))\n",
    "#teams['elo_name'].isnull().sum()\n",
    "\n",
    "elo_rating = pd.read_csv(os.getcwd() + '\\\\raw_data\\\\Elo_ratings\\\\elo_rating.csv', index_col=0)\n",
    "elo_rating.reset_index(inplace=True, drop=True)\n",
    "elo_rating['Name'] = clean_names2(elo_rating.Club)\n",
    "\n",
    "elo_rating.index = pd.IntervalIndex.from_arrays(elo_rating['From'], elo_rating['To'], closed='both')\n",
    "\n",
    "main_df['date_s'] = main_df['date'].str[:10]\n",
    "main_df['ELO_home'] = np.nan\n",
    "main_df['ELO_away'] = np.nan\n",
    "\n",
    "counter = 0 \n",
    "for team_name, elo_name  in zip(teams['team_long_name'], teams['elo_name']): \n",
    "    if str(elo_name) != 'nan':         \n",
    "        team_elo = elo_rating[elo_rating['Name']==elo_name]\n",
    "        main_df.loc[main_df['team_home']==team_name, 'ELO_home'] = \\\n",
    "            team_elo.iloc[team_elo.index.get_indexer(main_df\\\n",
    "            .loc[main_df['team_home']==team_name, 'date_s']), 4].values\n",
    "        main_df.loc[main_df['team_awat']==team_name, 'ELO_away'] = \\\n",
    "            team_elo.iloc[team_elo.index.get_indexer(main_df\\\n",
    "            .loc[main_df['team_awat']==team_name, 'date_s']), 4].values\n",
    "    counter += 1\n",
    "    if counter % 50 ==0: \n",
    "        print('Finished inserting elo for team', counter)\n",
    "del team_name, elo_name, counter, team_elo\n",
    "\n",
    "print('Share of nulls (Home ELO):', round(main_df['ELO_home'].isnull().sum()/len(main_df['ELO_home']),2))\n",
    "print('Share of nulls (Away ELO):', round(main_df['ELO_away'].isnull().sum()/len(main_df['ELO_away']),2))\n",
    "\n",
    "#main_df['ELO'] = main_df['date_s'].apply(lambda x: elo_rating.iloc[elo_rating.index.get_loc(x)]['Elo'])\n",
    "\n",
    "#Work\n",
    "#team_selector = main_df['team_home']=='KRC Genk'\n",
    "#team_elo = elo_rating[elo_rating['Club']=='Genk']\n",
    "##team_elo.loc[pd.idx.get_indexer(main_df.loc[main_df['team_home']=='KRC Genk', 'ELO_home'].date_s), 'Elo']\n",
    "##main_df.loc[team_selector, 'ELO_home'].apply(lambda x: team_elo.iloc[team_elo.index.get_loc(x)]['Elo'])\n",
    "#main_df.loc[main_df['team_home']=='KRC Genk', 'ELO_home'] = \\\n",
    "#    team_elo.iloc[team_elo.index.get_indexer(main_df\\\n",
    "#    .loc[main_df['team_home']=='KRC Genk', 'date_s']), 4].values\n",
    "\n",
    "del teams, teams_elo, elo_rating\n",
    "# main_df.drop(labels=['Unnamed: 0'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "### SAVE FILE\n",
    "main_df.to_csv('main_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
